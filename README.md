# üíß Water Potability Prediction Model

A machine learning project that predicts whether water is **safe for drinking** based on its chemical and physical properties.  
This notebook demonstrates the complete ML workflow ‚Äî from preprocessing and feature scaling to model training, evaluation, and hyperparameter tuning.

---

## üìò Project Overview

The goal is to determine the **potability** of water (1 = Potable, 0 = Not Potable) using the following parameters:

- pH  
- Hardness  
- Solids  
- Chloramines  
- Sulfate  
- Conductivity  
- Organic Carbon  
- Trihalomethanes  
- Turbidity  

The dataset contains around **3000 records** and represents a **binary classification** problem.

---

## üß† ML Pipeline

1. **Data Loading & Inspection**  
   - Imported the dataset and checked for missing values and distributions.  
   - Used mean imputation for normally distributed columns (like `ph`).

2. **Handling Skewness**  
   - Analyzed each feature‚Äôs distribution.  
   - Chose mean or median imputation depending on skewness type.

3. **Feature Encoding**  
   - The target (`Potability`) column was already in binary format (0 and 1), so no encoding was needed.

4. **Feature Scaling**  
   - Applied `StandardScaler` **only to input features**, not the output.  
   - Ensured all models received scaled data for consistent performance.

5. **Model Training & Evaluation**  
   - Trained and compared the following classifiers:
     - `LogisticRegression`
     - `SVC`
     - `KNeighborsClassifier`
     - `GaussianNB`
   - Evaluated models using **Accuracy**, **Precision**, **Recall**, and **F1-Score** on both train and test sets.

6. **Model Comparison**  
   - Stored evaluation metrics for all models in a DataFrame.  
   - Compared performance side-by-side.

7. **Hyperparameter Tuning**  
   - Used **RandomizedSearchCV** for faster hyperparameter tuning.  
   - Tuned all models with custom parameter grids.  
   - Accuracy remained stable due to limited dataset size (~3000 samples).

8. **Final Model Selection**  
   - The **Support Vector Classifier (SVC)** with an RBF kernel performed best.  
   - Stored as the `best_model` for further predictions.

9. **Custom Prediction Interface**  
   - Built a function to accept user inputs for new data.  
   - Inputs are converted into a NumPy array, reshaped using `reshape(1, -1)`, and scaled before prediction.  
   - Outputs whether the sample is **Potable** or **Not Potable**.

---

## üß© Example Custom Input

```python
custom_input = [7.2, 180.0, 15000.0, 6.5, 350.0, 400.0, 10.0, 80.0, 5.0]
input_scaled = scaler.transform(np.array(custom_input).reshape(1, -1))
prediction = best_model.predict(input_scaled)
```

Output:-
Predicted Class: Potable

---

## ‚öôÔ∏è Tech Stack

- **Python**
- **NumPy**
- **Pandas**
- **Matplotlib**
- **Seaborn**
- **scikit-learn**

---

## üìä Results Summary

| Model | Best CV Score | Test Accuracy | Precision | Recall | F1-Score |
|--------|---------------|---------------|------------|----------|-----------|
| Logistic Regression | ~0.64 | ~0.63 | Moderate | Moderate | Balanced |
| SVC (Tuned) | ~0.67 | ~0.66 | High | Good | Best |
| KNN | ~0.61 | ~0.60 | Moderate | Low | Low |
| Naive Bayes | ~0.62 | ~0.61 | Average | Moderate | Moderate |

---

## üí¨ Key Learnings

- For **normally distributed** columns like `ph`, using the **mean** to fill missing values works well.  
- For **skewed** columns, the **median** or transformations (like log-scaling) are better.  
- The **target column** didn‚Äôt require encoding since it was already in 0/1 format.  
- Always **standardize input features only**, never the output.  
- The **‚ÄúFitting 5 folds for each of 40 candidates‚Äù** line is auto-generated by `GridSearchCV` / `RandomizedSearchCV`.  
- `reshape(1, -1)` converts a 1D array into a 2D array, which is required for `predict()` methods.  
- Accuracy not improving after tuning is normal with smaller or well-behaved datasets.

---

## üöÄ Future Improvements

- Add ensemble models like **Random Forest**, **XGBoost**, or **AdaBoost**.  
- Save and load the final model using `joblib` or `pickle`.  
- Build an interactive **Streamlit web app** for real-time predictions.  
- Perform **feature importance analysis** and visualize correlations.  
- Explore **cross-validation** techniques for more robust evaluation.
